{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import cv2\n",
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio('data', output='train_test', ratio=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN layers (3 convolutional layers with max pooling, 1 dense layer, and a softmax output layer)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(300, 300, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Using generator to input the training and validation images. Also, used image augmentation on the training data.\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ").flow_from_directory('train_test/train',\n",
    "                      color_mode='grayscale',\n",
    "                      target_size=(300, 300),\n",
    "                      batch_size=128,\n",
    "                      class_mode='categorical')\n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    ").flow_from_directory('train_test/val',\n",
    "                      color_mode='grayscale',\n",
    "                      target_size=(300, 300),\n",
    "                      batch_size=16,\n",
    "                      class_mode='categorical')\n",
    "\n",
    "# Using checkpoints to save models\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model-{epoch:03d}.model', monitor='val_loss', verbose=0,\n",
    "                                                save_best_only=True, mode='auto')\n",
    "\n",
    "# Fitting the model on the data. Saved the model in a variable to compare results\n",
    "history = model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model-035.model')\n",
    "# Loading CV2 Haar cascade classifier\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# Using webcam\n",
    "source = cv2.VideoCapture(0)\n",
    "labels_dict = {1: 'Wearing a mask', 0: 'Not wearing a mask'}\n",
    "color_dict = {1: (0, 255, 0), 0: (0, 0, 255)}\n",
    "Door = {1: 'Door Opens', 0: 'Door Closed'}\n",
    "\n",
    "# Streaming from the webcam\n",
    "while True:\n",
    "    # Reading the image\n",
    "    ret, img = source.read()\n",
    "    # Rescaling the image to gray\n",
    "    grayscaled = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = classifier.detectMultiScale(grayscaled, 1.3, 5)\n",
    "\n",
    "    # Processing the image to fit the CNN\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = grayscaled[y:y + w, x:x + w]\n",
    "        face_img = cv2.resize(face_img, (300, 300))\n",
    "        face_img = face_img / 255.0\n",
    "        face_img = np.reshape(face_img, (1, 300, 300, 1))\n",
    "        result = model.predict(face_img)\n",
    "\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "        # Size of rectangle border around the face\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color_dict[label], 4)\n",
    "        cv2.rectangle(img, (x, y - 40), (x + w, y), color_dict[label], -1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.rectangle(img, (0, 0), (200,50), color_dict[label], 4)\n",
    "        cv2.rectangle(img, (0, 0), (200,50), color_dict[label], -1)\n",
    "        cv2.putText(img, Door[label], (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('LIVE', img)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    # Break when the esc key is pressed\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
